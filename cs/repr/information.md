# Information


Information is any entity or form that provides the answer to a question of some kind or resolves uncertainty. It is thus related to data and knowledge, as data represents values attributed to parameters, and knowledge signifies understanding of real things or abstract concepts.

A core precept of modern physics is that, in principle, information cannot be destroyed. It is also held that the existance of information does not depend on the consumer (for example, it exists beyond an event horizon of a black hole), while in the case of knowledge, the information requires a cognitive observer.


## Units of Information

A **unit of information** is the capacity of some standard data storage system or communication channel, used to measure the capacities of other systems and channels. In information theory, units of information are also used to measure the entropy of random variables and information contained in messages.

The funamental unit of data storage capacity (information) is the **bit**, the capacity of a system that has only 2 states. The name "bit" is portmanteau of "Binary digIT", they are not the same: a bit is the maximum amount of information that can be conveyed by a binary digit.

A **binary digit** is a number that can take one out of two possible values, either 0 or 1. By analogy, a binary digit is like a container, whereas information is the amount of matter in the container.

One bit is typically defined as the information entropy of a random binary variable that is 0 or 1 with equal probability, or the information that is gained when the value of such variable is consumed (i.e. when it becomes known).


> The most significant digit in a byte is bit#7 and the least significant digit is bit#0, otherwise known as **msb** and **lsb** respectively in lowercase; lsb is always bit#0, msb varies. **MSB** (uppercase) is the most significant byte.
